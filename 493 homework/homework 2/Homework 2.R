########################################################################################
#Tie Ma
#Student number 1537905
#class 439
#homework 2
########################################################################################
#Clean the enviroment
rm(list = ls())

#set the direction
setwd("~/SynologyDrive/nn/ECON-493-forcasting-economy/493 homework/homework 2")

#create a new file for all the generated by the graphic content
dir.create("pics")
#create a new file for all the data 
dir.create("pics")

#Lode the all package that necessary.
#some of the package may not been used in this homework
library("ggplot2")
library("fpp2")
library("glmnet")
library("tidyr")
library("lmtest")
library("boot")

#doctor who blue 
col = "#003b6f"

########################################################################################################################################################################
#Exercise 3

rm(list = ls())
#####
#3-a
#plot the data, Estimate a linear model with seasonal dummies as predictors using data from 1990Q1 to 2004Q4. 
#Evaluate the residuals. Compute the AIC and BIC.

au_beer <- window(ausbeer, start = 1990, end = c(2004,4))
fit.beer_season <- tslm(au_beer ~ season)
#time serive find the linear relationship 
#using the both trend with season

checkresiduals(fit.beer_season)
#According to esiduals graphy, we could find the residual is normally distribution
#but AFC graph suggest the residuals are autocorrelated which means the important variable is missing.le.


CV(fit.beer_season)
      #CV         AIC        AICc         BIC       AdjR2 
#220.9687500 282.3726654 283.6770132 292.1288840   0.9007409 

AIC(fit.beer_season)
  #546.8521
BIC(fit.beer_season)
  #557.3238


######
#3-b
  #Estimate a linear model with a trend and seasonal dummies as predictors 
  #using data from 1990Q1 to 2004Q4. Evaluate the residuals. Compute the AIC and BIC.

fit.beer_season_trend <- tslm(au_beer ~ season + trend)

###Evaluate the residuals
checkresiduals(fit.beer_season_trend)
# from the graphic we could find that the residuals are seem normal distributed,
#without any trend but according to ACF, all the autocorrelation coedficients
#lie almost and within the linite, the residuals are close to white noice. 

CV(fit.beer_season_trend)
#CV         AIC        AICc         BIC       AdjR2 
#192.7542573 274.3067186 276.1733853 286.0141809   0.9164696 

AIC(fit.beer_season_trend)
#519.3477
BIC(fit.beer_season_trend)
#531.9138



######
#3-c
#first of all, comparing the AIC/BIC, the linear regression model with both season
#and trend are better due to low AIC and BIC. second of all, by checking the residuals, 
#the linear regression model with both season and trend residuals are less autocorrelated
#and residuals do not seem to have a visible trend. 

#######
#3-e
#Evaluate the predictive performance of 
#these models in the test set 2005Q1 to 2009Q4.
#Which model performs better?

test_set <- window(ausbeer, start = 2005, end = c(2009,4))

#you cannot see which model is better from the graphy
evil_model_one <- forecast(fit.beer_season_trend)
evil_model_two <- forecast(fit.beer_season)

#autoplot(evil_model_one) + autolayer(evil_model_two, col = "red")

accuracy(evil_model_one, test_set)

                        #ME     RMSE      MAE       MPE     MAPE      MASE        ACF1 Theil s U
#Training set -1.776357e-15 16.59264 12.49218 -0.117834 2.792942 0.8106167 -0.04212879        NA
#Test set      1.206762e+01 16.80038 14.55452  2.984183 3.499996 0.9444419 -0.51518506 0.3360352


accuracy(evil_model_two, test_set)

                      #ME     RMSE      MAE        MPE     MAPE      MASE       ACF1 Theil's U
#Training set  1.894781e-15 21.21755 14.56444 -0.1973919 3.192709 0.9450856  0.2810504        NA
#Test set     -1.486667e+01 18.68071 15.18667 -3.4198192 3.499224 0.9854616 -0.4723262 0.3651066

<<<<<<< HEAD
cross valtion 
=======

>>>>>>> main

accuracy(evil_model_one, test_set)

                        #ME     RMSE      MAE       MPE     MAPE      MASE        ACF1 Theil s U
#Training set -1.776357e-15 16.59264 12.49218 -0.117834 2.792942 0.8106167 -0.04212879        NA
#Test set      1.206762e+01 16.80038 14.55452  2.984183 3.499996 0.9444419 -0.51518506 0.3360352


accuracy(evil_model_two, test_set)

                      #ME     RMSE      MAE        MPE     MAPE      MASE       ACF1 Theil's U
#Training set  1.894781e-15 21.21755 14.56444 -0.1973919 3.192709 0.9450856  0.2810504        NA
#Test set     -1.486667e+01 18.68071 15.18667 -3.4198192 3.499224 0.9854616 -0.4723262 0.3651066


#by compare two model's RMSE and MAE
  #we could find that the evil_model_one,which the linear regression with both season and trned
  #have smaller number of RMSE and MAE. Therefore the linear regreesion model with bot season 
  #and trend proforme better.


################################################################################################################################################################################
#Q4

#Q4-a Plot the data and ﬁnd the regression model for Demand 
#with temperature as an explanatory variable. Why is there a
#positive relationship?
#####
rm(list = ls())
dev.off()
######

daily20 <- head(elecdaily,20)
#######
#plot the data 
daily20.df <- daily20 %>% as.data.frame()
plot(daily20.df$Temperature,daily20.df$Demand)

#######
#find the regression model
model_one <- tslm(Demand ~ Temperature, data = daily20)
summary(model_one)


#Coefficients:
#  Estimate Std. Error t value Pr(>|t|)    
#(Intercept)  39.2117    17.9915   2.179   0.0428 *  
#  Temperature   6.7572     0.6114  11.052 1.88e-09 ***
  ---
#  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

#Residual standard error: 22 on 18 degrees of freedom
#Multiple R-squared:  0.8716,	Adjusted R-squared:  0.8644 
#F-statistic: 122.1 on 1 and 18 DF,  p-value: 1.876e-09


#why there are a positive relationship? 
  #the higher the tempture, people will spend more electricity on the keep them cooling

  
######
#Q4-b
plot(residuals(model_one))
checkresiduals(model_one)
#Form the residual diagnostics, we could find the residuals are not autocorrelated and it been normally distributed. However, 
#from the residual graph, we could find the residual is random therefore there are no outlier and influential observation.


######
#Q4-c

pure_evil <- data.frame(
  Temperature = c(35, 45)
)
daily_forecast_model <- forecast(model_one, newdata = pure_evil)
print(daily_forecast_model)

#Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95
#1       275.7146 245.2278 306.2014 227.5706 323.8586
#2       343.2868 310.3597 376.2140 291.2890 395.2846

#when the tempture was 15 degree
  #the forecast of usage of electricity is 275.7146

#when the tempture was 35 degree
  #the forecast of usage of electricity is 343.2868


######
#Q4-d
#When the temperature was 15 degree, the prediction intervals for 80% prediction interval accuracy is 
(306.2014 - 245.2278)/2 
# 30.4868

#When the temperature was 35 degree, the prediction intervals for 95% prediction interval is 
(323.8586 - 227.5706)/2
#48.144


#####
#Q4-e
View(elecdaily)
evil_data <-  head(elecdaily, 365)
evil_data.df <- evil_data %>% as.data.frame()
plot(evil_data.df$Temperatur, evil_data.df$Demand, xlab ="Temperatur" , ylab = "Demand", main = "elecdaily", col = "#003b6f")

# from the graphic we could find the the temperature and usage of demand have 
# the non-linear relationship. When the tempture within the rang among 0 to 20 degree
# the the usage and temperature have the negative relationship but when the temperature go 
# higher than 45, the usage and temperature have a positive relationship. therefore
# linear regression model cannot analysis this data set well enough. 


################################################################################################################################################################


#q-5
######## 
library(datasets)
library(forecast)
########
Huron<- window(LakeHuron, start=1875, end=1972)

#########
autoplot(Huron, col = "#003b6f") + xlab("Year") + ylab("depth")

# we could find both downward and cyclic patterns of the water's depth from graph.
# we cound find the a possible downward and cyclic path of the depth of the water 

########


# 5- b
#take of the year as the variale.
year <- time(Huron)
#fit a linear regression 
linear_regression_model <- tslm(Huron ~ year, data = Huron) 

#fit a piecewise linear trend
t.break1 <- 1915
t <- time(LakeHuron)
t1 <- ts(pmax(0, t-t.break1), start = 1915)
piecewise_linear_trend <- tslm(LakeHuron ~ t + t1)

autoplot(Huron) +
  autolayer(fitted(linear_regression_model), series = "Linear") +
  autolayer(fitted(piecewise_linear_trend), series = "Piecewise") +
  xlab("Year") +  ylab("depth") +
  guides(colour = guide_legend(title = " "))



############
#5-c
  
new_data_q5<- data.frame(
  year = c(1973:1980)
)
linear_regression_model_forecast <- forecast(linear_regression_model, newdata = new_data_q5)
      
t.new <- t[length(t)] + seq(8)
t1.new <- t1[length(t1)] + seq(8) 
newdata_evil<- data.frame("t" = t.new, "t1" = t1.new)
piecewise_linear_trend_forecast <- forecast(piecewise_linear_trend, newdata = newdata_evil)

autoplot(linear_regression_model_forecast, xlab = "Year", ylab = "depth", main = "linear_regression_model_forecast") 
autoplot(piecewise_linear_trend_forecast, series = "Piecewise", lab = "Year", ylab = "depth", main = "piecewise_linear_trend_forecast")
#comment 

#the linear regression model is forecasting there will be a downturn the piecewise forecasting are upward turn.


############
#5-d-1

Huron<- window(LakeHuron, start=1875, end=1972)
year <- time(Huron)
linear_regression_model <- tslm(Huron ~ year, data = Huron) 
t.break1 <- 1920
t <- time(LakeHuron)
t1 <- ts(pmax(0, t-t.break1), start = 1915)
piecewise_linear_trend <- tslm(LakeHuron ~ t + t1)
autoplot(Huron) +
  autolayer(fitted(linear_regression_model), series = "Linear") +
  autolayer(fitted(piecewise_linear_trend), series = "Piecewise") +
  xlab("Year") +  ylab("depth") +
  guides(colour = guide_legend(title = " "))

#the linear regression model display a downward trend
#the pieewise display a downward trend from 1875 - 1920 and after
#the time knot, it display a upward trend which is the diffetent when 
#the time knot set at 1915.


############
#5-d-2
new_data_q5<- data.frame( year = c(1973:1980))
linear_regression_model_forecast <- forecast(linear_regression_model, newdata = new_data_q5)
print(linear_regression_model_forecast)
autoplot(linear_regression_model_forecast)
t.new <- t[length(t)] + seq(8)
t1.new <- t1[length(t1)] + seq(8) 
newdata_evil<- data.frame("t" = t.new, "t1" = t1.new)
piecewise_linear_trend_forecast <- forecast(piecewise_linear_trend, newdata = newdata_evil)
autoplot(linear_regression_model_forecast,series = "Linear") 
autoplot(piecewise_linear_trend_forecast, series = "Piecewise")

#the linear regression model display a downward trend forecast, remain the same.
#the piecewise linear trend forecate a upward trend which is the different with 
#the time knot set at 1920 (which dieplay a downward forcasting).














