---
---
---

Tie Ma

Homework 3 - R code part

student number 1537905

```{r include=FALSE}
#Lode the all package that necessary.
#some of the package may not been used in this homework
library("ggplot2")
library("fpp2")
library("glmnet")
library("tidyr")
library("lmtest")
library("boot")
library("forecast")
library("readr")
library("ggfortify")
```

\###########################################

Exercise 3

3-a

```{r}
#Import the data
Coal_production<- window(bicoal, start=1920, end=1968)

#fit with the AR(4) model
Coal_production_AR4_model<- Arima(Coal_production, order = c(4,0,0))
```

This model is AR(4) model, therefore

p = 4, d = 0, and q = 0

3-b

```{r}
par(mfrow=c(1,2))
Acf(resid(arima(Coal_production, order=c(4, 0, 0))), main="ACF of Residuals")
Pacf(resid(arima(Coal_production, order=c(4, 0, 0))), main="PACF of Residuals")
```

By compare the ACF and PACF graphic, we could find the AR(4) or ARMA(4,0,0) include all the possible relationship such the residual left with white noise (smaller than 1). Therefore we should this model for explain the data.

3 - c

```{r}

#The mean adjust form
#MEAN ADJUSTFORM 

mu1 <- 0.83
mu2 <- -0.34
mu3 <- 0.55
mu4 <- -0.38
c <- 162.00

#calcuallate the variance
#μ = c/ 1 - φ1 - φ2 - φ3 -φ4


μ <- (c)/(1 - mu1 - mu2 - mu3 -mu4)
print(μ)
#μ <- 476.4706

y1968 <- 545
y1967 <-552
y1966 <-534
y1965 <-512
y1964 <-467
summary(c(y1967,y1968,y1966,y1965,y1964))
#mean: 522

#y1969

#y1969 <- μ + (φ1^q)*((Yt)- μ) + (φ2^q)*((Yt_-1)-μ) + (φ3^q)*((Yt_-2 )-μ) + (φ4^q)*((Yt_-3)- μ)

y1969 <- μ + mu1 * (y1968 - μ) + mu2 *(y1967-μ) + mu3 *(y1966-μ) + mu4 * (y1965 - μ)
print(y1969)
#1969
#525.81

#1970
y1970 <-  μ + ((mu1)^2) * (y1969 - μ) + ((mu2)^2) *(y1968- μ) + ((mu3)^2) *(y1967- μ) + ((mu4)^2) * (y1966 - μ)
print(y1970)
#y1970
#549.5374

#1971
y1971 <-  μ + (mu1^3) * (y1970 - μ) + (mu2^3) *(y1969-μ) + (mu3^3) *(y1968-μ) + (mu4^3) * (y1967 - μ)
print(y1971)
#y1971
#523.5671

plot(c(y1964, y1965, y1966, y1967, y1968, y1969, y1970, y1971))

```

3- d

```{r}
evil_forcast <- forecast(Arima(bicoal, order = c(4, 0, 0)))
plot(evil_forcast)

print(evil_forcast)
summary(Arima(bicoal, order = c(4, 0, 0)))

```

first, ARMA model will always went back to its mean in the long term. Consider the mean different between hand forecast and over data forecast.

second, for AR(4) model, the long term forcast always on the trend back to its expect value. Consider the the different mu, for example mu1 hand calculate is 0.84 but in AR(4) for all data set is 0.8334.

\#######

Exercise 4

4-a

```{r}
IBM_stock_price<- window(ibmclose)
summary(IBM_stock_price)
var(IBM_stock_price)
```

```{r}
#plot the data
plot(IBM_stock_price)
abline(h = 450, col = "red", lty = 2, lwd = 2)
text(1, 450, "The Average", pos = 4)
```

From the plot graphy, we can see a strong downward trend and a relative big varaince, therefor the data is not stationary.

```{r}
#Plot the ACF and PACF

#PACF AR SINIFICENT lag

par(mfrow=c(1,2))
Acf(IBM_stock_price)
Pacf(IBM_stock_price)
```

The non-stationary data ACF and PACF graphy will show the slow decaying trend of data collection. Therefore, the IBM stock market ACF graphic show that such data are non-stationary.

4-B

```{R}
#fit the model
IBM_stock_price_AR_1_model<- Arima(IBM_stock_price, order = c(1,0,0))

```

```{R}
#note: I want to try all the code that can give estimated coeffcient I knew. In case they have different reuslt and I lose the grade.In the end they all give the same result.
coef(IBM_stock_price_AR_1_model)
summary(IBM_stock_price_AR_1_model)
(fit2 <- Arima(IBM_stock_price, order = c(1,0,0)))
```

the estimated coefficient measures the correlation between the one observation and the observation last to it. the estimated coefficient measures the corrlation relationship between two observations. the positive estimated coefficient means the positive relationship and the negative estimated coefficient means the negative relationship. the larger the number, the stronger the relationship.

4-c

```{R}
#Use R to plot the ﬁrst diﬀerence of the series, the ACF and PACF. Explain how each plot shows that the diﬀerenced series is stationary.

IBM_stock_price_difference <- diff(IBM_stock_price)

summary(IBM_stock_price_difference)


plot(IBM_stock_price_difference)
abline(h = -0.2799, col = "red", lty = 2, lwd = 2)
text(1, -10, "The Average", pos = 4)
```

The plot does not display any visible trend and moving around the average compare the un-staational verson.

```{R}
par(mfrow=c(1,2))
Acf(IBM_stock_price_difference)
Pacf(IBM_stock_price_difference)

```

The ACF and PACF graphic does not show the slow decaying trend means that the observation have little or non relationship (correlation) with each others compare to the non-stationary data.

4-d

```{R}
IBM_stock_price_AR_1_model_differencedata<- Arima(IBM_stock_price_difference, order = c(1,0,0))

#since all three way will give the same estimated coeffcient, so I will use summary()

summary(IBM_stock_price_AR_1_model_differencedata)
```

The estimated coefficient is 0.0855 which is smaller than 0.9960 in the question 4-c. by different the data, the correlation relationship between observation have been massively decreased.

\####

Exercise 5

5-a

```{R}

GGDP_raw <- read_csv("/Users/tie/SynologyDrive/nn/ECON-493-forcasting-economy/493 homework/homework 3/NAEXKP01CAQ661S.csv", col_types = cols(NAEXKP01CAQ661S = col_number()))

plot(GGDP_raw, type = "l")
abline(h = 64.35, col = "red", lty = 2, lwd = 2)
text(1, 60, "The Average", pos = 4)

```

The plot data show the upward trend, therefore the data need to be stationary.

```{R message=TRUE, warning=TRUE}
par(mfrow=c(1,2))
Acf(GGDP_raw$NAEXKP01CAQ661S)
Pacf(GGDP_raw$NAEXKP01CAQ661S)

GGDP <- GGDP_raw
```

The non-stationary data ACF and PACF graphs will show a slow downward trend. In this data set, the ACF graph shows a slow downward trend, therefore, thus GDP data need to be stationary.

5-b

```{R}
#Use R to plot the change in log RGDP

log_different_GGPD <- diff(log(GGDP$NAEXKP01CAQ661S), lag = 1)
summary(log_different_GGPD)

plot(log_different_GGPD, type = "l")


```

The plot graph still have a down turn, which may suggest it is not fully stationary.

```{R}
par(mfrow=c(1,2))
Acf(log_different_GGPD)
Pacf(log_different_GGPD)
```

The ACF graphy suggest the same things. even there is a fast drop but the are slow and constant drop.

5-c

The ACF and PACF graph suggest both AR() and MA() (AKA: ARMA) because both graph display significant spikes.

For the ACF() graph, because the ACF graph is slowly drop and does not have spikes after 4th lag, therefore we need AR()

For the PACF() graph, because there are spikes exist, we need MA() model.

```{r}

#by using the function auto_arima() we cound just find the answer...
test_one <- auto.arima(log_different_GGPD)
print(test_one)
```

5-d

```{r echo=TRUE}
#This is a loop test from ARMA(1,1,0) to ARMA(10, 1, 0). choise 10 beacase the answer is 4 (see code in 3-c)

BIC_test <- matrix(0, nrow = 10, ncol = 2)

for(i in 1:10){BIC_test[i,1] <- i}

BIC_test[1,2] <- BIC(Arima(log_different_GGPD, order = c(1,1,0)))
BIC_test[2,2] <- BIC(Arima(log_different_GGPD, order = c(2,1,0)))
BIC_test[3,2] <- BIC(Arima(log_different_GGPD, order = c(3,1,0)))
BIC_test[4,2] <- BIC(Arima(log_different_GGPD, order = c(4,1,0)))
BIC_test[5,2] <- BIC(Arima(log_different_GGPD, order = c(5,1,0)))
BIC_test[6,2] <- BIC(Arima(log_different_GGPD, order = c(6,1,0)))
BIC_test[7,2] <- BIC(Arima(log_different_GGPD, order = c(7,1,0)))
BIC_test[8,2] <- BIC(Arima(log_different_GGPD, order = c(8,1,0)))
BIC_test[9,2] <- BIC(Arima(log_different_GGPD, order = c(9,1,0)))
BIC_test[10,2] <- BIC(Arima(log_different_GGPD, order = c(10,1,0)))

print(BIC_test)

```

According to the BIC test Matrix, the AR(3) has the lowest absolute BIC, therefore it is the best for AR() for describe model.

3-e

```{r}

AIC_test <- matrix(0, nrow = 10, ncol = 2)

for(i in 1:10){AIC_test[i,1] <- i}

AIC_test[1,2] <- AIC(Arima(log_different_GGPD, order = c(1,1,0)))
AIC_test[2,2] <- AIC(Arima(log_different_GGPD, order = c(2,1,0)))
AIC_test[3,2] <- AIC(Arima(log_different_GGPD, order = c(3,1,0)))
AIC_test[4,2] <- AIC(Arima(log_different_GGPD, order = c(4,1,0)))
AIC_test[5,2] <- AIC(Arima(log_different_GGPD, order = c(5,1,0)))
AIC_test[6,2] <- AIC(Arima(log_different_GGPD, order = c(6,1,0)))
AIC_test[7,2] <- AIC(Arima(log_different_GGPD, order = c(7,1,0)))
AIC_test[8,2] <- AIC(Arima(log_different_GGPD, order = c(8,1,0)))
AIC_test[9,2] <- AIC(Arima(log_different_GGPD, order = c(9,1,0)))
AIC_test[10,2] <- AIC(Arima(log_different_GGPD, order = c(10,1,0)))
                      
print(AIC_test)

```

According to AIC test matrix, the AR(2) has the lowest absolute value, therefore, according to AIC test, AT(2) is THE best adequately describe the chagne in log RGDP.

###
f. Use the models selected in parts (d) and (e) to forecast the quarterly log RGDP in 2018:Q2, 2018:Q3, and 2018:Q4. Compare your results.

Part d or BIC test show that AR(3) is the best model.
```{r}
Q5_f_d <- forecast(Arima(log_different_GGPD, order = c(3, 1, 0)))
plot(Q5_f_d)
print(Q5_f_d)
```


Part e or AIC test show that AR(2) is the best model
```{r}
Q5_f_e <- forecast(Arima(log_different_GGPD, order = c(2, 1, 0)))
autoplot(Q5_f_e)
print(Q5_f_e)
```


AIC
2018:Q2    0.02019323 -0.002515274 0.04290174 -0.0145364310 0.05492290
2018:Q3   0.02003051 -0.009778071 0.04983910 -0.0255577834 0.06561881
2018:Q4    0.01998481 -0.015721534 0.05569115 -0.0346233309 0.07459295

BIC

2018:Q2    0.02093293 -0.001697496 0.04356336 -0.0136773202 0.05554318
2018:Q3    0.02142733 -0.008562651 0.05141731 -0.0244383867 0.06729304
2018:Q4    0.02171863 -0.013722599 0.05715985 -0.0324840492 0.07592130



```{r}



```